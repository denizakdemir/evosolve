{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Distributional Optimization with TrainSelPy\n",
                "\n",
                "This notebook demonstrates the **distributional head** feature in TrainSelPy, which maintains a **distribution over solutions** rather than a single solution.\n",
                "\n",
                "## Key Concept\n",
                "\n",
                "Traditional optimization finds **one best solution**. Distributional optimization maintains a **weighted collection of solutions** (particles), enabling:\n",
                "- **Robustness** to uncertainty\n",
                "- **Risk management** through portfolio of solutions  \n",
                "- **Diverse strategies** for different scenarios\n",
                "\n",
                "## Use Case: Portfolio Selection Under Uncertainty\n",
                "\n",
                "Instead of selecting ONE portfolio, maintain a DISTRIBUTION of portfolios weighted by their performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from trainselpy.algorithms import initialize_population\n",
                "from trainselpy.distributional_head import (\n",
                "    ParticleDistribution,\n",
                "    compress_top_k,\n",
                "    compress_resampling,\n",
                "    mutate_weights,\n",
                "    mutate_support,\n",
                "    crossover_particle_mixture\n",
                ")\n",
                "from trainselpy.core import train_sel\n",
                "\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Create Market Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Asset Profiles:\n",
                        "Asset 1: Return=0.05, Risk=0.10\n",
                        "Asset 2: Return=0.08, Risk=0.15\n",
                        "Asset 3: Return=0.12, Risk=0.25\n",
                        "Asset 4: Return=0.07, Risk=0.12\n",
                        "Asset 5: Return=0.10, Risk=0.18\n",
                        "Asset 6: Return=0.06, Risk=0.08\n",
                        "Asset 7: Return=0.09, Risk=0.14\n",
                        "Asset 8: Return=0.11, Risk=0.20\n"
                    ]
                }
            ],
            "source": [
                "n_assets = 8\n",
                "asset_returns = np.array([0.05, 0.08, 0.12, 0.07, 0.10, 0.06, 0.09, 0.11])\n",
                "asset_risks = np.array([0.10, 0.15, 0.25, 0.12, 0.18, 0.08, 0.14, 0.20])\n",
                "\n",
                "print(\"Asset Profiles:\")\n",
                "for i in range(n_assets):\n",
                "    print(f\"Asset {i+1}: Return={asset_returns[i]:.2f}, Risk={asset_risks[i]:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Create Initial Particle Distribution\n",
                "\n",
                "First, generate a population of portfolio solutions, then create a distribution over them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Created 20 portfolio solutions\n",
                        "\n",
                        "Example portfolios:\n",
                        "  Portfolio 1: Assets [1 2 6 8]\n",
                        "  Portfolio 2: Assets [1 4 5 8]\n",
                        "  Portfolio 3: Assets [1 2 4 7]\n"
                    ]
                }
            ],
            "source": [
                "# Portfolio selection: choose 4 assets from 8\n",
                "candidates = [list(range(n_assets))]\n",
                "setsizes = [4]\n",
                "settypes = [\"UOS\"]  # Unordered set\n",
                "\n",
                "# Initialize population of portfolio solutions\n",
                "population = initialize_population(candidates, setsizes, settypes, pop_size=20)\n",
                "\n",
                "print(f\"Created {len(population)} portfolio solutions\")\n",
                "print(f\"\\nExample portfolios:\")\n",
                "for i in range(3):\n",
                "    selected = population[i].int_values[0]\n",
                "    print(f\"  Portfolio {i+1}: Assets {selected + 1}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Evaluate and Create Distribution\n",
                "\n",
                "Assign weights to portfolios based on their performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Particle Distribution:\n",
                        "  Number of particles: 20\n",
                        "  Weights sum: 1.0000\n",
                        "\n",
                        "Top 3 weighted portfolios:\n",
                        "  Weight=0.055: Assets [4 6 7 8]\n",
                        "  Weight=0.054: Assets [2 4 6 7]\n",
                        "  Weight=0.052: Assets [3 4 6 7]\n"
                    ]
                }
            ],
            "source": [
                "# Evaluate each portfolio\n",
                "fitness_scores = []\n",
                "for sol in population:\n",
                "    selected = sol.int_values[0]\n",
                "    portfolio_return = np.mean(asset_returns[selected])\n",
                "    portfolio_risk = np.mean(asset_risks[selected])\n",
                "    \n",
                "    # Sharpe-like ratio (return/risk)\n",
                "    fitness = portfolio_return / (portfolio_risk + 0.01)\n",
                "    fitness_scores.append(fitness)\n",
                "    sol.fitness = fitness\n",
                "\n",
                "# Create distribution: weight by fitness\n",
                "fitness_array = np.array(fitness_scores)\n",
                "weights = fitness_array / fitness_array.sum()\n",
                "\n",
                "# Create ParticleDistribution\n",
                "dist = ParticleDistribution(population, weights)\n",
                "\n",
                "print(f\"Particle Distribution:\")\n",
                "print(f\"  Number of particles: {dist.K}\")\n",
                "print(f\"  Weights sum: {dist.weights.sum():.4f}\")\n",
                "print(f\"\\nTop 3 weighted portfolios:\")\n",
                "top_indices = np.argsort(weights)[-3:][::-1]\n",
                "for idx in top_indices:\n",
                "    selected = population[idx].int_values[0]\n",
                "    print(f\"  Weight={weights[idx]:.3f}: Assets {selected + 1}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Distributional Objectives\n",
                "\n",
                "Different ways to evaluate a distribution of solutions:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "'numpy.ndarray' object is not callable",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m particle_fitness \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([p\u001b[38;5;241m.\u001b[39mfitness \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mparticles])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Compute distributional objectives\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m mean_perf \u001b[38;5;241m=\u001b[39m \u001b[43mmean_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparticle_fitness\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m mean_var_perf \u001b[38;5;241m=\u001b[39m mean_variance_objective(dist, particle_fitness, lambda_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     13\u001b[0m cvar_perf \u001b[38;5;241m=\u001b[39m cvar_objective(dist, particle_fitness, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
                        "File \u001b[0;32m~/Dropbox/dakdemirGithub/GitHubProjects/trainselpy/trainselpy/distributional_head.py:195\u001b[0m, in \u001b[0;36mmean_objective\u001b[0;34m(dist, base_fitness_fn, n_samples, data)\u001b[0m\n\u001b[1;32m    193\u001b[0m fitness_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sol \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[0;32m--> 195\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mbase_fitness_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdbl_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     fitness_values\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Return mean\u001b[39;00m\n",
                        "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
                    ]
                }
            ],
            "source": [
                "from trainselpy.distributional_head import (\n",
                "    mean_objective,\n",
                "    mean_variance_objective,\n",
                "    cvar_objective\n",
                ")\n",
                "\n",
                "# Extract fitness values\n",
                "particle_fitness = np.array([p.fitness for p in dist.particles])\n",
                "\n",
                "# Compute distributional objectives\n",
                "mean_perf = mean_objective(dist, particle_fitness)\n",
                "mean_var_perf = mean_variance_objective(dist, particle_fitness, lambda_var=0.5)\n",
                "cvar_perf = cvar_objective(dist, particle_fitness, alpha=0.1)\n",
                "\n",
                "print(\"Distributional Objective Values:\")\n",
                "print(f\"  Mean Performance: {mean_perf:.4f}\")\n",
                "print(f\"  Mean-Variance (λ=0.5): {mean_var_perf:.4f}\")\n",
                "print(f\"  CVaR (α=0.1): {cvar_perf:.4f}\")\n",
                "\n",
                "print(\"\\nInterpretation:\")\n",
                "print(\"  - Mean: Average fitness across distribution\")\n",
                "print(\"  - Mean-Variance: Balances performance and consistency\")\n",
                "print(\"  - CVaR: Focus on worst 10% of cases (tail risk)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Compression Strategies\n",
                "\n",
                "Reduce number of particles while preserving distribution quality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Original distribution: {dist.K} particles\")\n",
                "\n",
                "# Strategy 1: Keep top-k by weight\n",
                "dist_topk = compress_top_k(dist, k=5)\n",
                "print(f\"\\nTop-K (k=5): {dist_topk.K} particles\")\n",
                "print(f\"  Preserved weight: {dist_topk.weights.sum():.4f}\")\n",
                "\n",
                "# Strategy 2: Resampling\n",
                "dist_resample = compress_resampling(dist, k=5)\n",
                "print(f\"\\nResampling (k=5): {dist_resample.K} particles\")\n",
                "print(f\"  May have duplicates from resampling\")\n",
                "\n",
                "# Visualize weight distributions\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "axes[0].bar(range(dist.K), dist.weights)\n",
                "axes[0].set_title('Original (20 particles)')\n",
                "axes[0].set_ylabel('Weight')\n",
                "\n",
                "axes[1].bar(range(dist_topk.K), dist_topk.weights)\n",
                "axes[1].set_title('Top-K (5 particles)')\n",
                "\n",
                "axes[2].bar(range(dist_resample.K), dist_resample.weights)\n",
                "axes[2].set_title('Resampled (5 particles)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Distributional Operators\n",
                "\n",
                "Mutation and crossover for distributions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Weight mutation: Perturb weights\n",
                "dist_mut_weights = mutate_weights(dist, mutation_strength=0.1)\n",
                "\n",
                "print(\"Weight Mutation:\")\n",
                "print(f\"  Original weights: {dist.weights[:5]}\")\n",
                "print(f\"  Mutated weights: {dist_mut_weights.weights[:5]}\")\n",
                "\n",
                "# Support mutation: Modify particles themselves\n",
                "dist_mut_support = mutate_support(\n",
                "    dist, \n",
                "    candidates=candidates, \n",
                "    settypes=settypes, \n",
                "    mutation_prob=0.3\n",
                ")\n",
                "\n",
                "print(\"\\nSupport Mutation (modifies particles):\")\n",
                "print(f\"  Original particle 0: Assets {dist.particles[0].int_values[0] + 1}\")\n",
                "print(f\"  Mutated particle 0: Assets {dist_mut_support.particles[0].int_values[0] + 1}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Crossover: Combine Distributions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create second distribution\n",
                "pop2 = initialize_population(candidates, setsizes, settypes, pop_size=15)\n",
                "for sol in pop2:\n",
                "    selected = sol.int_values[0]\n",
                "    sol.fitness = np.mean(asset_returns[selected]) / (np.mean(asset_risks[selected]) + 0.01)\n",
                "\n",
                "fitness2 = np.array([s.fitness for s in pop2])\n",
                "weights2 = fitness2 / fitness2.sum()\n",
                "dist2 = ParticleDistribution(pop2, weights2)\n",
                "\n",
                "# Crossover: Mix two distributions\n",
                "offspring = crossover_particle_mixture(dist, dist2, mix_prob=0.5)\n",
                "\n",
                "print(f\"Crossover Results:\")\n",
                "print(f\"  Parent 1: {dist.K} particles\")\n",
                "print(f\"  Parent 2: {dist2.K} particles\")\n",
                "print(f\"  Offspring: {offspring.K} particles\")\n",
                "print(f\"\\nOffspring combines particles from both parents\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Practical Application: Asset Frequency Analysis\n",
                "\n",
                "Which assets appear most frequently in the distribution?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate weighted asset frequency\n",
                "asset_freq = np.zeros(n_assets)\n",
                "for particle, weight in zip(dist.particles, dist.weights):\n",
                "    selected = particle.int_values[0]\n",
                "    for asset in selected:\n",
                "        asset_freq[asset] += weight\n",
                "\n",
                "# Visualize\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Asset frequency\n",
                "colors = plt.cm.viridis(asset_freq / asset_freq.max())\n",
                "bars = ax1.bar(range(1, n_assets+1), asset_freq, color=colors, \n",
                "              edgecolor='black', linewidth=1.5)\n",
                "ax1.set_xlabel('Asset', fontsize=12)\n",
                "ax1.set_ylabel('Selection Frequency (weighted)', fontsize=12)\n",
                "ax1.set_title('Asset Frequency in Distribution', fontsize=13, fontweight='bold')\n",
                "ax1.grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Return vs frequency\n",
                "ax2.scatter(asset_freq, asset_returns, s=200, c=range(n_assets), \n",
                "           cmap='viridis', edgecolors='black', linewidth=2, alpha=0.7)\n",
                "for i in range(n_assets):\n",
                "    ax2.annotate(f'A{i+1}', (asset_freq[i], asset_returns[i]), \n",
                "                xytext=(5, 5), textcoords='offset points')\n",
                "ax2.set_xlabel('Selection Frequency', fontsize=12)\n",
                "ax2.set_ylabel('Expected Return', fontsize=12)\n",
                "ax2.set_title('Frequency vs Performance', fontsize=13, fontweight='bold')\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nInterpretation:\")\n",
                "print(\"  - High-frequency assets are robust choices\")\n",
                "print(\"  - Appear often across different high-performing portfolios\")\n",
                "print(f\"  - Most selected: Asset {np.argmax(asset_freq) + 1} ({asset_freq.max():.2f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Takeaways\n",
                "\n",
                "### Distributional Optimization Benefits:\n",
                "\n",
                "1. **Robustness**: Distribution hedges against uncertainty\n",
                "2. **Flexibility**: Different particles for different scenarios\n",
                "3. **Risk Management**: CVaR objective focuses on tail risk\n",
                "4. **Interpretability**: Frequency indicates robustness\n",
                "\n",
                "### When to Use:\n",
                "\n",
                "- **High uncertainty** environments\n",
                "- **Risk-averse** applications\n",
                "- **Multiple scenarios** to handle\n",
                "- Need **portfolio of strategies**\n",
                "\n",
                "### Distributional Objectives:\n",
                "\n",
                "- `mean`: Maximize average performance\n",
                "- `mean_variance`: Balance performance and consistency\n",
                "- `cvar`: Protect against worst cases"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
